{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative PygmalionAI notebook for Google Colab\n",
        "\n",
        "Version: GPU\n",
        "\n",
        "This notebook is an unofficial method to run [PygmalionAI LLMs](https://github.com/PygmalionAI) that uses **oobabooga**'s [text-generation-webui project](https://github.com/oobabooga/text-generation-webui).\n",
        "\n",
        "Run the cells below in order to set it up.\n",
        "\n",
        "---\n",
        "\n",
        "After step 3, if you enabled Google Drive, you may upload character definitions in `.json` format by navigating to:\n",
        "  * [**Drive**](https://drive.google.com/) &rarr; **My Drive** &rarr; **Colab-TextGen** &rarr; **characters**\n",
        "\n",
        "At the end of step 5 click on the `.gradio.live` link to access the web interface."
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "# From: https://github.com/henk717/KoboldAI\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://henk.tech/colabkobold/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Prepare the Conda environment\n",
        "\n",
        "%cd /tmp\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "%env PYTHONPATH=\n",
        "!which conda 2>/dev/null || bash Miniconda3-latest-Linux-x86_64.sh -b -p /usr/local -f\n",
        "%env CI=true\n",
        "!conda init -q bash\n",
        "!source /usr/local/bin/activate && conda update -y -q -c defaults --all\n",
        "!source /usr/local/bin/activate && conda install -y -q -c defaults conda python\n",
        "!source /usr/local/bin/activate && conda install -y -q -c nvidia cudatoolkit\n",
        "!source /usr/local/bin/activate && conda install -y -q -c pytorch -c \"nvidia/label/cuda-11.7.0\" pytorch torchvision torchaudio pytorch-cuda=11.7 cuda-cudart cuda-nvprof\n",
        "!source /usr/local/bin/activate && conda clean -y -q --all"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Clone the repository and install dependencies\n",
        "\n",
        "Enable_Google_Drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if Enable_Google_Drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "# Download Github repo\n",
        "!git -C /content/drive/MyDrive/Colab-TextGen pull || git clone https://github.com/oobabooga/text-generation-webui /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "# Set model directories outside of Google Drive\n",
        "!mountpoint -q models || mv -f models models_backup\n",
        "!mountpoint -q torch-dumps || mv -f torch-dumps torch-dumps_backup\n",
        "!mkdir -p models torch-dumps /content/models /content/torch-dumps\n",
        "!mountpoint -q models || mount --bind /content/models models\n",
        "!mountpoint -q torch-dumps || mount --bind /content/torch-dumps torch-dumps\n",
        "\n",
        "# Install requirements\n",
        "!source /usr/local/bin/activate \\\n",
        "  && pip install --no-cache-dir --progress-bar=off --root-user-action=ignore -r requirements.txt \\\n",
        "  && pip install --no-cache-dir --progress-bar=off --root-user-action=ignore diffusers"
      ],
      "metadata": {
        "id": "F1mssJmZ5BMg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Select a PygmalionAI model\n",
        "\n",
        "Model = \"Pygmalion 6B\" #@param [\"Pygmalion 6B\", \"Pygmalion 6B experimental\", \"Pygmalion 2.7B\", \"Pygmalion 1.3B\", \"Pygmalion 350M\"] {allow-input: false}\n",
        "Reshard_model = False #@param {type: \"boolean\"}\n",
        "Max_shard_size = \"3GB\" #@param [\"5GB\", \"3GB\", \"1GB\"] {allow-input: true}\n",
        "\n",
        "if Model == \"Pygmalion 6B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-6b\"\n",
        "elif Model == \"Pygmalion 6B experimental\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-6b\"\n",
        "  huggingface_branch = \"dev\"\n",
        "  model_name = \"pygmalion-6b_dev\"\n",
        "elif Model == \"Pygmalion 2.7B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-2.7b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-2.7b\"\n",
        "elif Model == \"Pygmalion 1.3B\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-1.3b\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-1.3b\"\n",
        "elif Model == \"Pygmalion 350M\":\n",
        "  huggingface_org = \"PygmalionAI\"\n",
        "  huggingface_repo = \"pygmalion-350m\"\n",
        "  huggingface_branch = \"main\"\n",
        "  model_name = \"pygmalion-350m\"\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "# Download the model\n",
        "![[ ! -f models/$model_name/config.json ]] \\\n",
        "  && source /usr/local/bin/activate \\\n",
        "  && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "# Convert model if the reshard option was checked\n",
        "if Reshard_model:\n",
        "  original_path = model_name\n",
        "  resharded_path = model_name + f'_resharded'\n",
        "  launch_model = resharded_path\n",
        "else:\n",
        "    launch_model = model_name\n",
        "![[ $Reshard_model = True ]] \\\n",
        "  && source /usr/local/bin/activate \\\n",
        "  && wget -O reshard-model.py https://gist.githubusercontent.com/81300/fe5b08bff1cba45296a829b9d6b0f303/raw/b4d6f8971e2c4f03b183e5bdeb8f7be0b424b865/reshard-model.py \\\n",
        "  && python reshard-model.py --src-path models/$original_path --out-path models/$resharded_path --max-shard-size $Max_shard_size --offload /content/offload \\\n",
        "  && cp -fa models/$original_path/*token*.json models/$original_path/vocab.json models/$resharded_path"
      ],
      "metadata": {
        "id": "yyl9TGSoRIwV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Launch the web interface\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab-TextGen\n",
        "\n",
        "!source /usr/local/bin/activate \\\n",
        "  && python server.py --share --cai-chat --no-stream --auto-devices --model $launch_model"
      ],
      "metadata": {
        "id": "txeAxCGyRK1F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cleanup all models\n",
        "\n",
        "#@markdown Free up space within the current runtime. After this completes run step 4 again.\n",
        "\n",
        "!find /content/models -mindepth 1 -maxdepth 1 -type d -exec rm -rv \"{}\" \\;\n",
        "!find /content/torch-dumps -mindepth 1 -exec rm -rv \"{}\" \\;"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XbKanChesbUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}